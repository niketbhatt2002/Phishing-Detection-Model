{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxv0vprLqnj-",
        "outputId": "2626ae7e-6367-44ad-bd79-b3bba70d39d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=e1918c04b02abb21a9e5ac08bedd9fa36d2fe4dc17b3a7723122fd877f766e55\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Read the dataset with a specific encoding\n",
        "input_csv_file = 'spam.csv'  # Replace with the path of your dataset\n",
        "\n",
        "# Try different encodings if necessary\n",
        "df = pd.read_csv(input_csv_file, encoding='ISO-8859-1')\n",
        "\n",
        "# Function to translate text to Telugu\n",
        "def translate_to_telugu(text):\n",
        "    try:\n",
        "        translated = translator.translate(text, src='en', dest='te')\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error translating text: {e}\")\n",
        "        return text\n",
        "\n",
        "# Translate only the first 500 rows of the 'v2' column\n",
        "df_first_500 = df.head(500)  # Select the first 500 rows\n",
        "df_first_500['v2'] = df_first_500['v2'].apply(translate_to_telugu)\n",
        "\n",
        "# Save the updated DataFrame with translated 'v2' column to a new CSV\n",
        "output_csv_file = 'telugu_translated_first_500_rows.csv'  # New output CSV file with Telugu text in 'v2'\n",
        "df_first_500.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"CSV file with first 500 rows translated and saved as {output_csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_U6ieqCqpR1",
        "outputId": "cb440561-5df9-4bfe-ab5a-0b7ee1b0bedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file with first 500 rows translated and saved as telugu_translated_first_500_rows.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ae1f6b8073b8>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_first_500['v2'] = df_first_500['v2'].apply(translate_to_telugu)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ✅ Load CSV file with correct encoding\n",
        "df = pd.read_csv(\"telugu_translated_first_500_rows.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# ✅ Ensure only needed columns are present\n",
        "df = df[['v1', 'v2']]  # Keeping only 'v1' and 'v2'\n",
        "\n",
        "# ✅ Create an empty list to store modified rows\n",
        "new_data = []\n",
        "\n",
        "i = 0\n",
        "while i < len(df):\n",
        "    new_data.append(df.iloc[i])  # Add the current row\n",
        "\n",
        "    if df.iloc[i]['v1'].lower() == 'spam':\n",
        "        new_data.append(df.iloc[i])  # Duplicate the spam row\n",
        "\n",
        "        if i + 1 < len(df) and df.iloc[i + 1]['v1'].lower() == 'ham':\n",
        "            i += 1  # Skip the next ham row\n",
        "\n",
        "    i += 1  # Move to the next row\n",
        "\n",
        "# ✅ Convert list back to DataFrame\n",
        "new_df = pd.DataFrame(new_data)\n",
        "\n",
        "# ✅ Save the updated file\n",
        "new_df.to_csv(\"balanced_translated.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"✅ Done! File saved as 'balanced_translated.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2USw58Mq6P7",
        "outputId": "aa838c9c-ce3b-4b3a-e13d-73335624f012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! File saved as 'balanced_translated.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ✅ Replace with your actual API key and region\n",
        "API_KEY = \"FrN1fqSKqGS60hCdVaCuy46V63oF7Kp5M8dz3EJ8q3eDrkzG4jVPJQQJ99BBACYeBjFXJ3w3AAAbACOGzZGM\"  # <-- Paste your Key 1 here\n",
        "LOCATION = \"eastus\"  # Example: \"westeurope\", \"eastus\", etc.\n",
        "\n",
        "# Microsoft Translator API endpoint\n",
        "ENDPOINT = \"https://api.cognitive.microsofttranslator.com/translate\"\n",
        "\n",
        "# ✅ Load your CSV file with a different encoding\n",
        "df = pd.read_csv(\"spam.csv\", encoding='ISO-8859-1')  # Try 'ISO-8859-1' or 'utf-16'\n",
        "\n",
        "# ✅ Select only the relevant columns (v1 for labels, v2 for sentences)\n",
        "df = df[['v1', 'v2']]\n",
        "\n",
        "# ✅ Function to translate text to Telugu\n",
        "def translate_to_telugu(text):\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "\n",
        "    params = {\n",
        "        \"api-version\": \"3.0\",\n",
        "        \"from\": \"en\",\n",
        "        \"to\": \"te\"  # Change 'hi' to 'te' for Telugu\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Ocp-Apim-Subscription-Key\": API_KEY,\n",
        "        \"Ocp-Apim-Subscription-Region\": LOCATION,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    body = [{\"text\": text}]\n",
        "\n",
        "    response = requests.post(ENDPOINT, params=params, headers=headers, json=body)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[0][\"translations\"][0][\"text\"]\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}\"  # Handle API errors\n",
        "\n",
        "# ✅ Translate only the first 501 rows in column 'v2' (the sentences) into Telugu\n",
        "df.loc[:500, 'v2'] = df.loc[:500, 'v2'].apply(translate_to_telugu)\n",
        "\n",
        "# ✅ Save only the first 501 rows (with both 'v1' and translated 'v2') into a new CSV file\n",
        "df_first_501 = df.head(501)  # Get only the first 501 rows\n",
        "\n",
        "df_first_501.to_csv(\"translated_dataset_telugu_first_501_rows.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"✅ Translation completed for the first 501 rows! File saved as 'translated_dataset_telugu_first_501_rows.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMBhthqfs3_y",
        "outputId": "946f919d-5c75-4e80-ad50-372a110b7b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Translation completed for the first 501 rows! File saved as 'translated_dataset_telugu_first_501_rows.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ✅ Load CSV file with correct encoding\n",
        "df = pd.read_csv(\"translated_dataset_telugu_first_501_rows.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# ✅ Ensure only needed columns are present\n",
        "df = df[['v1', 'v2']]  # Keeping only 'v1' and 'v2'\n",
        "\n",
        "# ✅ Create an empty list to store modified rows\n",
        "new_data = []\n",
        "\n",
        "i = 0\n",
        "while i < len(df):\n",
        "    new_data.append(df.iloc[i])  # Add the current row\n",
        "\n",
        "    if df.iloc[i]['v1'].lower() == 'spam':\n",
        "        new_data.append(df.iloc[i])  # Duplicate the spam row\n",
        "\n",
        "        if i + 1 < len(df) and df.iloc[i + 1]['v1'].lower() == 'ham':\n",
        "            i += 1  # Skip the next ham row\n",
        "\n",
        "    i += 1  # Move to the next row\n",
        "\n",
        "# ✅ Convert list back to DataFrame\n",
        "new_df = pd.DataFrame(new_data)\n",
        "\n",
        "# ✅ Save the updated file\n",
        "new_df.to_csv(\"Telugu_microsoft_balanced_translated.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"✅ Done! File saved as 'balanced_translated.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRyiy5veuGE3",
        "outputId": "b0a23913-52a5-4a2e-bf1e-d2cba42f8ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! File saved as 'balanced_translated.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vC1YO3yYupgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}