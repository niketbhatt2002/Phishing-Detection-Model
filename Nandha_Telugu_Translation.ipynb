{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA2pY_okRQt2",
        "outputId": "c9921076-deac-414a-9f8f-e20b68452bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=68649d5c8f35fe6324eb47f65dc7966e50d1e9f278186fbe158c9802d329adb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yflO_M1sPNai",
        "outputId": "a47b2e1f-db54-44a5-a498-d7c9a63969e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error translating text: list index out of range\n",
            "CSV file successfully translated and saved as telugu_translated_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Read the dataset with a specific encoding\n",
        "input_csv_file = 'spam.csv'  # Replace with the path of your dataset\n",
        "\n",
        "# Try different encodings if necessary\n",
        "df = pd.read_csv(input_csv_file, encoding='ISO-8859-1')\n",
        "\n",
        "# Function to translate text to Telugu\n",
        "def translate_to_telugu(text):\n",
        "    try:\n",
        "        translated = translator.translate(text, src='en', dest='te')\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error translating text: {e}\")\n",
        "        return text\n",
        "\n",
        "# Translate only the 'v2' column\n",
        "df['v2'] = df['v2'].apply(translate_to_telugu)\n",
        "\n",
        "# Save the updated DataFrame with translated 'v2' column\n",
        "output_csv_file = 'telugu_translated_dataset.csv'  # Output CSV file with Telugu text in 'v2'\n",
        "df.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"CSV file successfully translated and saved as {output_csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame with 'v1' as labels and 'v2' as text data\n",
        "# Replace this with your actual DataFrame load process\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Separate the ham and spam rows\n",
        "ham_df = df[df['v1'] == 'ham']\n",
        "spam_df = df[df['v1'] == 'spam']\n",
        "\n",
        "# Duplicate the spam rows to make their count 1447 (currently 747, so we need 700 more)\n",
        "spam_duplicated = pd.concat([spam_df] * 2, ignore_index=True)\n",
        "\n",
        "# Now we have 1447 spam rows\n",
        "# Randomly sample 700 rows from ham to drop\n",
        "ham_reduced = ham_df.sample(n=4125, random_state=42)\n",
        "\n",
        "# Combine the modified ham and spam DataFrames\n",
        "balanced_df = pd.concat([ham_reduced, spam_duplicated])\n",
        "\n",
        "# Shuffle the resulting DataFrame to mix the rows\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# If you want to save the resulting balanced DataFrame to a CSV\n",
        "balanced_df.to_csv('new_dataset.csv', index=False)\n",
        "\n",
        "# Display the balanced dataset's class distribution\n",
        "print(balanced_df['v1'].value_counts())\n"
      ],
      "metadata": {
        "id": "eoAn9Fc1RZEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba14600b-dc02-42c3-c34c-6f2f72794244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v1\n",
            "ham     4125\n",
            "spam    1494\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# ✅ Replace with your actual API key and region\n",
        "API_KEY = \"FrN1fqSKqGS60hCdVaCuy46V63oF7Kp5M8dz3EJ8q3eDrkzG4jVPJQQJ99BBACYeBjFXJ3w3AAAbACOGzZGM\"  # <-- Paste your Key 1 here\n",
        "LOCATION = \"eastus\"  # Example: \"westeurope\", \"eastus\", etc.\n",
        "\n",
        "# Microsoft Translator API endpoint\n",
        "ENDPOINT = \"https://api.cognitive.microsofttranslator.com/translate\"\n",
        "\n",
        "# ✅ Load your CSV file with a different encoding\n",
        "df = pd.read_csv(\"spam.csv\", encoding='ISO-8859-1')  # Try 'ISO-8859-1' or 'utf-16'\n",
        "\n",
        "# ✅ Select only the relevant columns (v1 for labels, v2 for sentences)\n",
        "df = df[['v1', 'v2']]\n",
        "\n",
        "# ✅ Inspect the first few rows of the dataframe and the column names\n",
        "print(df.head())  # This will print the first few rows\n",
        "print(df.columns)  # This will print the column names\n",
        "\n",
        "# ✅ Function to translate text to Telugu\n",
        "def translate_to_telugu(text):\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "\n",
        "    params = {\n",
        "        \"api-version\": \"3.0\",\n",
        "        \"from\": \"en\",\n",
        "        \"to\": \"te\"  # Change 'hi' to 'te' for Telugu\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Ocp-Apim-Subscription-Key\": API_KEY,\n",
        "        \"Ocp-Apim-Subscription-Region\": LOCATION,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    body = [{\"text\": text}]\n",
        "\n",
        "    response = requests.post(ENDPOINT, params=params, headers=headers, json=body)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[0][\"translations\"][0][\"text\"]\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}\"  # Handle API errors\n",
        "\n",
        "# ✅ Translate all rows in column 'v2' (the sentences) into Telugu\n",
        "df['v2'] = df['v2'].apply(translate_to_telugu)\n",
        "\n",
        "# ✅ Save the updated DataFrame (now only Telugu text in v2, with the original labels intact in v1)\n",
        "df.to_csv(\"translated_dataset_telugu.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"✅ Translation completed! English removed, only Telugu retained in the 'v2' column. File saved as 'translated_dataset_telugu.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKMCNSWnFUFr",
        "outputId": "9fa467b6-b0cd-47fc-cc6a-f2fb777e1555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "Index(['v1', 'v2'], dtype='object')\n",
            "✅ Translation completed! English removed, only Telugu retained in the 'v2' column. File saved as 'translated_dataset_telugu.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLSmltfBkyEB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}